<div align="center">

<div style="font-size: 36px; font-weight: bold;">
<span style="color: darkgreen;">Uni</span><span style="color: darkred;">Morph</span><span style="color: darkblue;">Grasp</span>: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation
</div>

<br>

<img src="figs/teaser.gif" alt="Teaser Image" width="80%">
<div style="width: 80%; margin: 0 auto; text-align: center;">
We present UniMorphGrasp, a diffusion model with morphology-awareness that can generate <span style="color: #0070C0; font-weight: bold;">diverse cross-embodiment grasps</span> and <span style="color: #0070C0; font-weight: bold;">generalize to novel morphologies</span>.
</div>

<br>
<br>
<br>
<br>

<div style="font-size: 24px; font-weight: bold;">Abstract</div>
<br>
<div style="width: 80%; margin: 0 auto; text-align: justify;">
Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with varying structures. Existing methods are either hand-specific, computationally prohibitive, or fail to generalize beyond the training distribution when encountering novel hand structures. Motivated by the observation that dexterous hands inherently possess graph-structured morphologies, we propose <span style="color: #0070C0; font-weight: bold;">UniMorphGrasp</span>, a morphology-aware diffusion model that integrates explicit morphological information into the generative process for cross-embodiment dexterous grasp synthesis. Our approach first maps diverse hand structures into a unified human-like hand representation, and then employs a morphology-aware encoder that conditions grasp generation on graph-structured morphological features. We further introduce a morphology-aware loss function that leverages hierarchical kinematic relationships to guide training. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing benchmarks and successfully <span style="color: #0070C0; font-weight: bold;">generalizes to novel hand structures in a zero-shot way</span>, enabling practical cross-embodiment grasp deployment.
</div>

<br>
<br>
<hr> <br>
<br>

<div style="font-size: 24px; font-weight: bold;">Method</div>
<br>
<img src="figs/pipeline.png" alt="Pipeline Image" width="90%">

<div style="width: 90%; margin: 0 auto; text-align: justify;">
<span style="color: #0070C0; font-weight: bold;">(Left)</span> The overview of our proposed UniMorphGrasp for cross-embodiment dexterous grasp generation. Given an object point cloud and an arbitrary hand morphology extracted from its URDF specification (mapped to a pre-defined canonical hand format), we employ a morphology encoder to extract morphology representations from the hand's joint structure. The hand pose (noised via a diffusion scheduler in training) is embedded through a linear layer, and concatenated with its active joint mask embedding to obtain the hand representation. This representation is then processed through a morphology-aware denoising model, where the iterative process is conditioned on both the morphology representation and the point cloud representation extracted via a Point Transformer. The entire framework is trained using a morphology-aware loss function. <span style="color: #0070C0; font-weight: bold;">(Right)</span> The structure of our morphology-aware denoising model, which is conditioned on the encoded morphology and the point cloud representations via cross-attention.
</div>

<br>
<br>
<hr> <br>
<br>

<div style="font-size: 24px; font-weight: bold;">Method Performance</div>
<br>

<img src="figs/multidex.gif" alt="MultiDex Performance" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
UniMorphGrasp can generate stable and diverse grasps for cross-embodiment dexterous hands.
</div>

<br><br>

<img src="figs/more_multidex.gif" alt="More MultiDex Results" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
More results generated by our UniMorphGrasp on the MultiDex dataset.
</div>

<br><br>

<img src="figs/quantitative1.png" alt="Quantitative Results 1" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
Quantitative comparison of our UniMorphGrasp (w/. and w/o. the morphology-aware loss) with different cross-embodiment dexterous grasp synthesis baselines across three robotic hands from three to five fingers: Barrett, Allegro, and Shadow hand.
</div>

<br><br>

<img src="figs/qualitative.gif" alt="Qualitative Comparison" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
Qualitative comparison with baselines 1) GenDexGrasp and 2) DRO-Grasp, where our results demonstrate superior surface conformity and stable form-closure.
</div>

<br>
<br>
<hr> <br>
<br>

<div style="font-size: 24px; font-weight: bold;">Zero-Shot Generalization to Novel Hand Morphologies</div>
<br>

<img src="figs/generalization1.gif" alt="Generalization 1" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
<span style="color: #0070C0; font-weight: bold;">Topological Variations</span>: We selectively remove fingers of the Shadow hand.
</div>

<br><br>

<img src="figs/generalization2.gif" alt="Generalization 2" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
<span style="color: #0070C0; font-weight: bold;">Geometrical Variations</span>: We scale the finger lengths by factors of 1.5× (lengthened).
</div>

<br><br>

<img src="figs/generalization3.gif" alt="Generalization 3" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
<span style="color: #0070C0; font-weight: bold;">Geometrical Variations</span>: We scale the finger lengths by factors of 0.8× (shortened).
</div>

<br><br>

<img src="figs/generalization4.gif" alt="Generalization 4" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
<span style="color: #0070C0; font-weight: bold;">Embodiment Variations</span>: We replace Shadow Hand fingers with Allegro Hand fingers to introduce embodiment changes in joint axis, joint limits, and link geometries.
</div>

<br>
<br>
<hr> <br>
<br>

<div style="font-size: 24px; font-weight: bold;">Cross-Dataset Results</div>
<br>
<div style="width: 90%; margin: 0 auto; text-align: justify;">
We conduct <span style="color: #0070C0; font-weight: bold;">cross-dataset evaluations</span> on the Multi-GraspLLM and Objaverse datasets to evaluate the zero-shot generalization capability of our model.
</div>
<br>

<img src="figs/multi-graspllm.gif" alt="Multi GraspLLM" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
Visualizations of cross-embodiment grasps synthesized by UniMorphGrasp on the Multi-GraspLLM dataset.
</div>

<br><br>

<img src="figs/objaverse.gif" alt="Objaverse Results" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
Visualizations of cross-embodiment grasps synthesized by UniMorphGrasp on the Objaverse dataset.
</div>

<br><br>

<img src="figs/quantitative2.png" alt="Quantitative Results 2" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
Cross-dataset zero-shot generalization results. We evaluate models trained on MultiDex directly on unseen datasets: Multi-GraspLLM and Objaverse.
</div>

<br>
<br>
<hr> <br>
<br>

<div style="font-size: 24px; font-weight: bold;">Real-World Experiments</div>
<br>
<div style="width: 90%; margin: 0 auto; text-align: justify;">
We validate UniMorphGrasp in real-world scenarios using a UR5e arm equipped with a Leap Hand.
</div>
<br>

<img src="figs/real-world1.gif" alt="Real World 1" width="90%">
<img src="figs/real-world2.gif" alt="Real World 2" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
Real-world grasping demonstrations on the Leap Hand.
</div>

<br><br>

<img src="figs/quantitative3.png" alt="Quantitative Results 3" width="90%">
<div style="width: 90%; margin: 0 auto; text-align: center;">
Quantitative real-world evaluation on the Leap Hand. We report the success rate over 10 attempts for eight objects from the YCB dataset.
</div>

</div>
